
官网原文地址：https://docs.influxdata.com/influxdb/v1.2/concepts/storage_engine/

The InfluxDB Storage Engine and the Time-Structured Merge Tree (TSM)：

新的InfluxDB存储引擎看起来非常类似于LSM树。 它具有预写日志和一组只读数据文件，这些文件在概念上类似于LSM树中的SSTable。 TSM文件包含已排序的压缩系列数据。

InfluxDB将为每个时间块创建一个分片。 例如，如果您的保留策略具有无限期，则将为每7天的时间段创建分片。 这些分片中的每一个都映射到底层存储引擎数据库。 每个数据库都有自己的WAL和TSM文件。

我们将深入研究存储引擎的每个部分。


Storage Engine：
存储引擎将多个组件连接在一起，并提供用于存储和查询系列数据的外部接口。它由许多组件组成，每个组件都发挥特定的作用：

内存中索引 - 内存中索引是跨分片的共享索引，可快速访问度量，标记和序列。索引由引擎使用，但不是特定于存储引擎本身。
WAL - WAL是一种写入优化的存储格式，允许写入持久，但不容易查询。写入WAL将附加到固定大小的段。
高速缓存 - 高速缓存是存储在WAL中的数据的内存中表示。它在运行时查询并与存储在TSM文件中的数据合并。
TSM文件 - TSM文件以列式格式存储压缩系列数据。
FileStore - FileStore调解对磁盘上所有TSM文件的访问。它确保在替换现有TSM文件时以原子方式安装TSM文件以及删除不再使用的TSM文件。
压缩机 - 压缩机负责将优化较少的Cache和TSM数据转换为更多读取优化的格式。它通过压缩序列，删除已删除的数据，优化索引以及将较小的文件组合成较大的文件来实现。
压缩计划程序 - 压缩计划程序确定哪些TSM文件已准备好进行压缩，并确保多个并发压缩不会相互干扰。
压缩 - 压缩由各种编码器和解码器处理，用于特定数据类型。有些编码器是相当静态的，并且总是以相同的方式编码相同的类型;其他人根据数据的形状切换压缩策略。
Writers/Readers - 每种文件类型（WAL段，TSM文件，墓碑等）都有写Writers/Readers来处理格式。


Write Ahead Log(WAL):
WAL被组织为一堆看起来像_000001.wal的文件。 文件号单调增加，称为WAL段。 当段大小达到10MB时，它将关闭并打开一个新段。 每个WAL段存储多个压缩的写入和删除块。

当写入时，新的点被序列化，使用Snappy压缩，并写入WAL文件。 该文件是fsync'd，并且在返回成功之前将数据添加到内存中索引。 这意味着，配料点就是需要实现高吞吐量性能。 （对于许多用例，最佳批量大小似乎是每批5,000-10,000点。）

WAL中的每个条目都遵循TLV标准，其中单个字节表示条目的类型（写入或删除），4字节uint32表示压缩块的长度，然后是压缩块。


Cache:
高速缓存是当前存储在WAL中的所有数据点的内存中副本。点由键组织，即测量，标记集和唯一字段。每个字段都保留为自己的时间顺序范围。缓存数据在内存中不会被压缩。

对存储引擎的查询将来自Cache的数据与来自TSM文件的数据合并。在查询处理时从缓存中对数据副本执行查询。这种在查询运行时写入的方式不会影响结果。

发送到缓存的删除将清除给定密钥或给定密钥的特定时间范围。

缓存为快照行为公开了一些控件。两个最重要的控件是内存限制。有一个下限，cache-snapshot-memory-size，当超出时会触发快照到TSM文件并删除相应的WAL段。还有一个上限，cache-max-memory-size，当超出时会导致Cache拒绝新写入。这些配置对于防止内存不足以及向客户端写入数据的压力比实例可以持续存储的速度更快。
每次写入都会检查内存阈值。

其他快照控件是基于时间的。如果空闲阈值cache-snapshot-write-cold-duration在指定的时间间隔内没有收到写入，则会强制Cache将快照复制到TSM文件。

通过重新读取磁盘上的WAL文件，在重新启动时重新创建内存中的缓存。


Compression:
在查询时，每个块都经过压缩以减少存储空间和磁盘IO。 块包含给定系列和字段的时间戳和值。 每个块都有一个字节的头，然后是压缩的时间戳，然后是压缩的值。
┌───────┬─────┬─────────────────┬──────────────────┐
│ Type  │ Len │   Timestamps    │      Values      │
│1 Byte │VByte│     N Bytes     │     N Bytes      │
└───────┴─────┴─────────────────┴──────────────────┘
使用取决于数据类型及其形状的编码来分别压缩和存储时间戳和值。 独立存储它们允许时间戳编码用于所有时间戳，同时允许不同字段类型的不同编码。 例如，某些点可能能够使用行程编码，而其他点可能不会。

每个值类型还包含一个1字节的头，表示剩余字节的压缩类型。 四个高位存储压缩类型，如果需要，编码器使用四个低位。


Timestamps:

1569/5000
时间戳编码是自适应的并且基于编码的时间戳的结构。它使用delta编码，缩放和压缩的组合，使用simple8b游程编码，并在需要时回退到无压缩。

时间戳分辨率是可变的，但可以像纳秒一样精细，需要最多8个字节来存储未压缩。在编码期间，值首先进行增量编码。第一个值是起始时间戳，后续值是与先前值的差异。这通常会将值转换为更容易压缩的更小的整数。许多时间戳也单调增加，甚至落后于时间边界，例如每10秒。当时间戳具有这种结构时，它们按最大公约数进行缩放，该除数也是10的因子。这具有将非常大的整数增量转换为压缩得更好的较小整数增量的效果。

使用这些调整后的值，如果所有增量相同，则使用行程编码存储时间范围。如果不能进行游程编码并且所有值都小于（1 << 60） - 1（纳秒分辨率下约36。5年），则使用simple8b编码对时间戳进行编码。 Simple8b编码是一种64位字对齐的整数编码，它将多个整数打包成一个64位字。如果任何值超过最大值，则使用块的每个8字节对未加压的增量进行存储。未来的编码可以使用补丁方案，例如Patched Frame-Of-Reference（PFOR）来更有效地处理异常值。


Floats:
Floats使用Facebook Gorilla纸张的实现进行编码。 当值靠近时，编码将连续值XOR一起产生小的结果。 然后使用控制位存储增量，以指示XOR值中有多少前导零和尾随零。 我们的实现删除了纸上描述的时间戳编码，只编码浮点值。


Integers:
整数编码根据未压缩数据中的值范围使用两种不同的策略。 首先使用ZigZag编码对编码值进行编码。 这在一系列正整数上交错正整数和负整数。

例如，[ - 2，-1,0,1]变为[3,1,0,2]。 有关详细信息，请参阅Google的协议缓冲区文档。

如果所有ZigZag编码值都小于（1 << 60） - 1，则使用simple8b编码对它们进行压缩。 如果任何值大于最大值，则所有值都在块中未压缩地存储。 如果所有值都相同，则使用行程编码。
这对于经常不变的值非常有效。


Booleans:
布尔值使用简单的位打包策略进行编码，其中每个布尔值使用1位。 编码的布尔值的数量在块的开头使用可变字节编码来存储。


Strings:
字符串使用Snappy压缩进行编码。 每个字符串连续打包，并将它们压缩为一个较大的块。


Compactions:
压缩是反复进行的过程，将以写优化格式存储的数据迁移到更加读取优化的格式。当碎片热写入时，会发生许多压缩阶段：

快照 - 必须将Cache和WAL中的值转换为TSM文件，以释放WAL段使用的内存和磁盘空间。这些压缩基于高速缓存存储器和时间阈值而发生。
级别压缩 - 级别压缩（级别1-4）随着TSM文件的增长而发生。 TSM文件从快照压缩到1级文件。压缩多个1级文件以生成2级文件。此过程将继续，直到文件达到级别4和TSM文件的最大大小。
除非需要运行删除，索引优化压缩或完全压缩，否则不会进一步压缩它们。较低级别的压缩使用策略来避免CPU密集型活动，如解压缩和组合块。较高级别（因此较不频繁）的压缩将重新组合块以完全压缩它们并提高压缩比。
索引优化 - 当许多4级TSM文件累积时，内部索引变得越来越大，访问成本也越来越高。索引优化压缩在一组新的TSM文件中拆分系列和索引，将给定系列的所有点分类到一个TSM文件中。在索引优化之前，每个TSM文件包含大多数或所有系列的点，因此每个TSM文件包含相同的系列索引。在索引优化之后，每个TSM文件包含来自最少系列的点，并且文件之间几乎没有系列重叠。因此，每个TSM文件具有较小的唯一系列索引，而不是完整系列列表的副本。此外，特定系列中的所有点在TSM文件中是连续的，而不是分布在多个TSM文件中。
完全压缩 - 当碎片长时间写入冷却时，或者在碎片上发生删除时，将执行完全压缩。完全压缩产生一组最佳TSM文件，并包括来自级别和索引优化压缩的所有优化。一旦碎片完全压缩，除非存储新的写入或删除，否则不会对其运行任何其他压缩。


Writes:
写入被附加到当前WAL段，并且还被添加到高速缓存中。 每个WAL段都有最大尺寸。 当前文件填满后，写入滚动到新文件。 缓存也是大小有限的; 当缓存变得太满时，将执行快照并启动WAL压缩。 如果入站写入速率超过WAL压缩率持续一段时间，则缓存可能变得太满，在这种情况下，新写入将失败，直到快照进程赶上。

当WAL段填满并关闭时，Compactor会对Cache进行快照并将数据写入新的TSM文件。 成功写入TSM文件并进行fsync后，它将被FileStore加载并引用。


Updates:
更新（为已存在的点写入更新的值）作为正常写入发生。 由于缓存值会覆盖现有值，因此较新的写入优先。 如果写入将覆盖先前TSM文件中的点，则在查询运行时合并这些点，并且较新的写入优先。


Deletes:
删除是通过将一个删除条目写入WAL进行测量或系列，然后更新Cache和FileStore而发生的。 缓存驱逐所有相关条目。 FileStore为包含相关数据的每个TSM文件写入一个逻辑删除文件。 这些逻辑删除文件在启动时用于忽略块以及在压缩期间删除已删除的条目。

在查询时处理针对部分删除的系列的查询，直到压缩从TSM文件中完全删除数据。



Queries:
当存储引擎执行查询时，它实质上是对与特定系列键和字段相关联的给定时间的搜索。 首先，我们搜索数据文件以查找包含与查询匹配的时间范围的文件以及包含匹配系列的文件。

一旦我们选择了数据文件，接下来我们需要在系列键索引条目的文件中找到位置。 我们对每个TSM索引运行二进制搜索以找到其索引块的位置。

在常见情况下，块不会跨多个TSM文件重叠，我们可以线性搜索索引条目以找到要从中读取的起始块。 如果存在重叠的时间块，则对索引条目进行排序以确保新的写入优先，并且可以在查询执行期间按顺序处理块。

当迭代索引条目时，从块部分顺序读取块。 该块被解压缩，我们寻求特定点。


The new InfluxDB storage engine: from LSM Tree to B+Tree and back again to create the Time Structured Merge Tree:
编写新的存储格式应该是最后的选择。那么InfluxData最终是如何编写我们自己的引擎的呢？ InfluxData已经尝试了许多存储格式，发现每种格式都缺乏一些基本的方式。 InfluxDB的性能要求非常高，最终会压倒其他存储系统。 InfluxDB的0.8行允许多个存储引擎，包括LevelDB，RocksDB，HyperLevelDB和LMDB。 InfluxDB的0.9行使用BoltDB作为底层存储引擎。这篇文章是关于0.9.5发布的时间结构合并树存储引擎，是InfluxDB 0.11+支持的唯一存储引擎，包括整个1.x系列。

时间序列数据用例的属性使其对许多现有存储引擎具有挑战性。在InfluxDB的开发过程中，我们尝试了一些比较流行的选项。我们从LevelDB开始，这是一个基于LSM树的引擎，它针对写入吞吐量进行了优化。之后我们尝试了BoltDB，这是一个基于内存映射B + Tree的引擎，它针对读取进行了优化。最后，我们最终建立了自己的存储引擎，它在很多方面与LSM树类似。

通过我们的新存储引擎，我们能够将B + Tree设置的磁盘空间使用量降低45倍，并且比我们在LevelDB及其变体中看到的写入吞吐量和压缩率更高。这篇文章将介绍这一演变的细节，最后深入探讨我们的新存储引擎及其内部工作原理。


Properties if Time Series Data:
时间序列数据的工作量与普通数据库工作负载完全不同。有许多因素导致难以扩展并保持高效：

数十亿个人数据点
高写入吞吐量
高读取吞吐量
大删除（数据到期）
主要是插入/追加工作负载，很少更新
第一个也是最明显的问题是规模问题。在DevOps，IoT或APM中，每天可以轻松收集数亿或数十亿个唯一数据点。

例如，假设我们有200台虚拟机或服务器正在运行，每台服务器平均每10秒收集100次测量。鉴于一天中有86,400秒，一次测量每台服务器每天将产生8,640个点。这使我们每天总共获得200 * 100 * 8,640 = 172,800,000个人数据点。我们在传感器数据用例中发现相似或更大的数字。

数据量意味着写入吞吐量可能非常高。我们经常收到设置请求，而不是每秒处理数十万次写入。一些大公司只会考虑每秒可处理数百万次写入的系统。

同时，时间序列数据可以是高读取吞吐量用例。确实，如果您正在跟踪700,000个独特的指标或时间序列，那么您无法想象所有这些指标或时间序列。这导致许多人认为您实际上并未阅读进入数据库的大部分数据。然而，除了人们在屏幕上显示的仪表板之外，还有用于监视或组合大量时间序列数据与其他类型数据的自动化系统。

在InfluxDB内部，即时计算的聚合函数可以将数万个不同的时间序列组合成单个视图。这些查询中的每一个都必须读取每个聚合数据点，因此对于InfluxDB，读取吞吐量通常比写入吞吐量高许多倍。

鉴于时间序列主要是仅附加工作负载，您可能会认为可以在B +树上获得出色的性能。键空间中的附加效率很高，每秒可以达到100,000以上。但是，我们会在各个时间序列中发生这些追加。因此，插入最终看起来更像是随机插入，而不是仅附加插入。

我们在时间序列数据中发现的最大问题之一是，在经过一定年龄后删除所有数据是很常见的。这里的常见模式是用户具有高精度数据，可以在短时间内保存，例如几天或几个月。然后，用户下采样并将该数据聚合到保持更长时间的较低精度汇总中。

天真的实现是在每个记录通过其到期时间后简单地删除它们。但是，这意味着一旦写入的第一个点达到其到期日期，系统就会处理与写入一样多的删除，这是大多数存储引擎不适合的。

让我们深入研究我们尝试的两种类型的存储引擎的细节，以及这些属性如何对我们的性能产生重大影响。


LevelDB and Log Structured Merge Trees:
当InfluxDB项目开始时，我们选择了LevelDB作为存储引擎，因为我们已经将它用于作为InfluxDB前身的产品中的时间序列数据存储。我们知道它具有很好的写入吞吐量属性，而且一切似乎都“正常工作”。

LevelDB是日志结构化合并树（或LSM树）的一种实现，它是作为Google的开源项目构建的。它为密钥空间排序的键/值存储公开API。最后一部分对时间序列数据很重要，因为只要时间戳在密钥中，我们就可以快速扫描时间范围。

LSM树基于一个采用写入的日志和两个称为Mem Tables和SSTables的结构。这些表表示已排序的键空间。 SSTables是只读文件，不断被其他SSTable替换，这些SSTable将插入和更新合并到键空间中。

LevelDB为我们带来的两大优势是高写入吞吐量和内置压缩。然而，随着我们更多地了解人们对时间序列数据的需求，我们遇到了一些难以克服的挑战。

我们遇到的第一个问题是LevelDB不支持热备份。如果要对数据库进行安全备份，则必须将其关闭然后复制。 LevelDB变种RocksDB和HyperLevelDB解决了这个问题，但还有另一个更紧迫的问题，我们认为它们无法解决。

我们的用户需要一种自动管理数据保留的方法。这意味着我们需要大规模删除。在LSM树中，删除与写入一样昂贵，如果不是更多的话。删除会写一个称为逻辑删除的新记录。之后，查询将结果集与任何逻辑删除器合并，以从查询返回中清除已删除的数据。稍后，压缩运行会删除SSTable文件中的逻辑删除记录和基础删除记录。

为了绕过删除，我们将数据分割成我们称为分片的数据，这些分片是连续的时间块。碎片通常会保存一天或七天的数据。每个分片都映射到底层LevelDB。这意味着我们可以通过关闭数据库并删除基础文件来删除一整天的数据。

RocksDB的用户可能会在此时调出一个名为ColumnFamilies的功能。将时间序列数据放入Rocks时，通常会将时间块分成列族，然后在时间结束时删除它们。这是一个相同的概念：创建一个单独的区域，您可以在删除大块数据时删除文件而不是更新索引。删除列族是一种非常有效的操作。但是，列族是一个相当新的功能，我们有另一个用于分片的用例。

将数据组织成分片意味着它可以在群集中移动而无需检查数十亿个密钥。在撰写本文时，无法将一个RocksDB中的列族移动到另一个。旧的碎片通常很冷，因此移动它们会很便宜。我们还有一个额外的好处，就是在密钥空间中有一个写入冷的位置，以便稍后进行一致性检查会更容易。

将数据组织成分片的工作时间很长，直到大量数据进入InfluxDB。 LevelDB将数据分成许多小文件。在一个过程中打开数十个或数百个这样的数据库最终会产生一个大问题。拥有六个月或一年数据的用户将耗尽文件句柄。这不是我们在大多数用户中找到的东西，但是任何将数据库推到极限的人都会遇到这个问题而我们没有解决它。打开的文件句柄太多了。



BoltDB and mmap B+Trees:
在与LevelDB及其变体争吵了一年后，我们决定转向BoltDB，这是一个纯粹受LMDB启发的纯Golang数据库，这是一个用C语言编写的mmap B + Tree数据库。它具有与LevelDB相同的API语义：一个关键值存储键空间的排序位置。我们的许多用户都感到惊讶。我们自己发布的LevelDB变体与LMDB（mmap B + Tree）的测试表明RocksDB是最佳表现者。

但是，除了纯写入性能之外，还有其他考虑因素。在这一点上，我们最重要的目标是获得可以在生产中运行并备份的稳定的东西。 BoltDB还具有使用纯Go编写的优势，这极大地简化了我们的构建链，并使其易于为其他操作系统和平台构建。

对我们来说最大的胜利是BoltDB使用单个文件作为数据库。在这一点上，我们最常见的错误报告来源是人们用完文件句柄。博尔特同时解决了热备份问题和文件限制问题。

我们愿意在写入吞吐量方面受到打击，如果这意味着我们有一个更可靠和稳定的系统，我们可以建立。我们的理由是，对于任何推动真正大量写入负载的人来说，无论如何他们都会运行一个集群。

我们基于BoltDB发布了0.9.0到0.9.2版本。从发展的角度来看，这是令人愉快的。清洁API，在我们的Go项目中快速轻松地构建，并且可靠。但是，运行一段时间后，我们发现写入吞吐量存在很大问题。数据库超过几GB后，写入将开始激活IOPS。

有些用户可以通过将InfluxDB放在几乎无限IOPS的大硬件上来解决这个问题。但是，大多数用户都位于云中资源有限的虚拟机上。我们必须找到一种方法来减少将一堆点数一次写入数十万个系列的影响。

在0.9.3和0.9.4版本中，我们的计划是在Bolt前放置一个写入日志（WAL）。这样我们就可以减少随机插入键空间的次数。相反，我们将缓冲多个彼此相邻的写入，然后立即刷新它们。但是，这只能延迟问题。高IOPS仍然成为一个问题，对于任何在中等工作负载下运行的人来说，它都会很快出现。

但是，我们在Bolt前面构建第一个WAL实现的经验给了我们所需的信心，可以解决写入问题。 WAL本身的表现太棒了，指数根本无法跟上。在这一点上，我们再次开始考虑如何创建类似于LSM树的东西，以便能够跟上我们的写入负载。

因此诞生了时间结构合并树。